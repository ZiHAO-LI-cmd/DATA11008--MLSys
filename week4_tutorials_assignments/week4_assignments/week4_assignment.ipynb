{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4 Assignments\n",
    "In this week's assignments, you'll gain more hands-on experience with deploying ML models, especially using KServe. \n",
    "\n",
    "### Prerequisite: \n",
    "To do this week's assignments, we assume you have previously trained a LightGBM regression model for bike sharing demand prediction and have already uploaded the model artifact to your MLflow service (week1 assignments). If you haven't completed this step, uncomment and run the next code cell before proceeding to the assignments for this week.\n",
    "\n",
    "### Guidelines for submitting the assignments\n",
    "- As usual, please submit this assignment notebook with code cell outputs. It's important that these outputs are current and reflect the latest state of your code, as your grades may depend on them.\n",
    "- Unlike the assignments of the previous weeks where you write Python code in the assignment notebooks, you will need to fill some configurations in JSON/YAML files in most of the assignments. In other words, you will need to put your answers in the required JSON/YAML files (not in this notebook, you can use the commands in the notebooks to check if you're progressing correctly).\n",
    "- For submission, please also include these JSON/YAML files in your submission. More precisely, these files include `model-settings.json` in the \"assignment1\" directory and `*.yaml` in the \"manifests\" directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisite\n",
    "\n",
    "# from train_helper import train\n",
    "# params = {\n",
    "#     \"num_leaves\": 63,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"random_state\": 42\n",
    "# }\n",
    "# train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Use MLServer to deploy a model locally (2 points)\n",
    "[MLServer](https://mlserver.readthedocs.io/en/latest/index.html) is an open-source inference server implementation for ML models. It provides an easy way to expose a model through an HTTP or gRPC endpoint. \n",
    "\n",
    "You already trained a LightGBM model for predicting bike sharing demand and upload it to the MLflow service in the first week. In this assignment, your task is to configure MLServer to serve your LightGBM model as an inference service locally. Detailed instructions can be found later. \n",
    "\n",
    "**Hints**:\n",
    "- Reading the following MLServer documentation may be enough to complete the assignment:\n",
    "    - [Getting started with MLServer](https://mlserver.readthedocs.io/en/latest/getting-started/index.html#). You'll see an example of using MLServer SDK to implement a custom model server in this documentation. You don't need to implement your own model server to complete this assignment as MLServer has an out-of-box inference server implementation for models registered to MLflow (see the second documentation). \n",
    "    - [Serving MLflow models](https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the dependency requirements of running the LightGBM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your own LightGM model's S3 URI\n",
    "model_s3_uri = \"s3://mlflow/5/c946adf98fd348a1bf993121f8133893/artifacts/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/26 20:49:21 INFO mlflow.pyfunc: To install the dependencies that were used to train the model, run the following command: '%pip install -r /tmp/tmp6zns_zje/model/requirements.txt'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp6zns_zje/model/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os \n",
    "\n",
    "# Configure MLflow\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "\n",
    "# Configure the credentials needed for accessing the MinIO storage service\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "# Download the requirements.txt file of the model and print the file's location\n",
    "file_path = mlflow.pyfunc.get_model_dependencies(model_uri=model_s3_uri)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move requirements.txt to the \"assignment1\" directory\n",
    "!mv {file_path} ./assignment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not mess up the \"mlops_eng\" environment, let's create a new Python environment named \"try_mlserver\" and install the dependencies. Open a new terminal and run the following commands\n",
    "```bash\n",
    "conda create -n try_mlserver -yf python==3.10 ipykernel\n",
    "conda activate try_mlserver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Switch the Python environment of this notebook to the new \"try_mlserver\" environment** by clicking the current environment name at the upper right corner and install the dependencies by running the following two code cells. \n",
    "\n",
    "If you can't find the environment name, close VS Code and open it again, then reopen the notebook. This may force VS Code to detect all available Python environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow==2.3 (from -r assignment1/requirements.txt (line 1))\n",
      "  Using cached mlflow-2.3.0-py3-none-any.whl (17.7 MB)\n",
      "Collecting argparse==1.4.0 (from -r assignment1/requirements.txt (line 2))\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: cffi==1.15.1 in /home/user/.local/lib/python3.10/site-packages (from -r assignment1/requirements.txt (line 3)) (1.15.1)\n",
      "Collecting cloudpickle==2.2.1 (from -r assignment1/requirements.txt (line 4))\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting defusedxml==0.7.1 (from -r assignment1/requirements.txt (line 5))\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting lightgbm==3.3.5 (from -r assignment1/requirements.txt (line 6))\n",
      "  Downloading lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib==3.8.0 (from -r assignment1/requirements.txt (line 7))\n",
      "  Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting numpy==1.26.0 (from -r assignment1/requirements.txt (line 8))\n",
      "  Using cached numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Collecting packaging==23.0 (from -r assignment1/requirements.txt (line 9))\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.5.3 (from -r assignment1/requirements.txt (line 10))\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil==5.9.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from -r assignment1/requirements.txt (line 11)) (5.9.0)\n",
      "Collecting scikit-learn==1.3.2 (from -r assignment1/requirements.txt (line 12))\n",
      "  Using cached scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy==1.10.1 (from -r assignment1/requirements.txt (line 13))\n",
      "  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "Collecting click<9,>=7.0 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting databricks-cli<1,>=0.8.7 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting entrypoints<1 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting gitpython<4,>=2.1.0 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /home/user/.local/lib/python3.10/site-packages (from mlflow==2.3->-r assignment1/requirements.txt (line 1)) (6.0)\n",
      "Collecting protobuf<5,>=3.12.0 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting pytz<2024 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting requests<3,>=2.17.3 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting importlib-metadata!=4.7.0,<7,>=3.7.0 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached alembic-1.12.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<7,>=4.0.0 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting Flask<3 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting querystring-parser<2 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached SQLAlchemy-2.0.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting pyarrow<12,>=4.0.0 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gunicorn<21 (from mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/user/.local/lib/python3.10/site-packages (from mlflow==2.3->-r assignment1/requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: pycparser in /home/user/.local/lib/python3.10/site-packages (from cffi==1.15.1->-r assignment1/requirements.txt (line 3)) (2.21)\n",
      "Requirement already satisfied: wheel in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from lightgbm==3.3.5->-r assignment1/requirements.txt (line 6)) (0.41.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.8.0->-r assignment1/requirements.txt (line 7))\n",
      "  Using cached contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.8.0->-r assignment1/requirements.txt (line 7))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.8.0->-r assignment1/requirements.txt (line 7))\n",
      "  Using cached fonttools-4.45.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (155 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib==3.8.0->-r assignment1/requirements.txt (line 7))\n",
      "  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib==3.8.0->-r assignment1/requirements.txt (line 7))\n",
      "  Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.8.0->-r assignment1/requirements.txt (line 7))\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from matplotlib==3.8.0->-r assignment1/requirements.txt (line 7)) (2.8.2)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn==1.3.2->-r assignment1/requirements.txt (line 12))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.3.2->-r assignment1/requirements.txt (line 12))\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached Mako-1.3.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4 (from alembic!=1.10.0,<2->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyjwt>=1.7.0 (from databricks-cli<1,>=0.8.7->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting oauthlib>=3.1.0 (from databricks-cli<1,>=0.8.7->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting tabulate>=0.7.7 (from databricks-cli<1,>=0.8.7->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.3->-r assignment1/requirements.txt (line 1)) (1.16.0)\n",
      "Collecting urllib3<3,>=1.26.7 (from databricks-cli<1,>=0.8.7->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting websocket-client>=0.32.0 (from docker<7,>=4.0.0->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached websocket_client-1.6.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting Werkzeug>=2.3.7 (from Flask<3->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask<3->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting blinker>=1.6.2 (from Flask<3->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: setuptools>=3.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from gunicorn<21->mlflow==2.3->-r assignment1/requirements.txt (line 1)) (68.0.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/.local/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.3->-r assignment1/requirements.txt (line 1)) (2.1.3)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.17.3->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.17.3->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.17.3->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy<3,>=1.4.0->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.3->-r assignment1/requirements.txt (line 1))\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "Using cached alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
      "Using cached docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "Using cached flask-2.3.3-py3-none-any.whl (96 kB)\n",
      "Using cached fonttools-4.45.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Using cached Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "Using cached protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached SQLAlchemy-2.0.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached websocket_client-1.6.4-py3-none-any.whl (57 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Using cached Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, argparse, zipp, Werkzeug, websocket-client, urllib3, typing-extensions, threadpoolctl, tabulate, sqlparse, smmap, querystring-parser, pyparsing, pyjwt, protobuf, pillow, packaging, oauthlib, numpy, markdown, Mako, kiwisolver, joblib, itsdangerous, idna, gunicorn, greenlet, fonttools, entrypoints, defusedxml, cycler, cloudpickle, click, charset-normalizer, certifi, blinker, sqlalchemy, scipy, requests, pyarrow, pandas, importlib-metadata, gitdb, Flask, contourpy, scikit-learn, matplotlib, gitpython, docker, databricks-cli, alembic, mlflow, lightgbm\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "Successfully installed Flask-2.3.3 Mako-1.3.0 Werkzeug-3.0.1 alembic-1.12.1 argparse-1.4.0 blinker-1.7.0 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-2.2.1 contourpy-1.2.0 cycler-0.12.1 databricks-cli-0.18.0 defusedxml-0.7.1 docker-6.1.3 entrypoints-0.4 fonttools-4.45.1 gitdb-4.0.11 gitpython-3.1.40 greenlet-3.0.1 gunicorn-20.1.0 idna-3.6 importlib-metadata-6.8.0 itsdangerous-2.1.2 joblib-1.3.2 kiwisolver-1.4.5 lightgbm-3.3.5 markdown-3.5.1 matplotlib-3.8.0 mlflow-2.3.0 numpy-1.26.0 oauthlib-3.2.2 packaging-23.0 pandas-1.5.3 pillow-10.1.0 protobuf-4.25.1 pyarrow-11.0.0 pyjwt-2.8.0 pyparsing-3.1.1 pytz-2023.3.post1 querystring-parser-1.2.4 requests-2.31.0 scikit-learn-1.3.2 scipy-1.10.1 smmap-5.0.1 sqlalchemy-2.0.23 sqlparse-0.4.4 tabulate-0.9.0 threadpoolctl-3.2.0 typing-extensions-4.8.0 urllib3-2.1.0 websocket-client-1.6.4 zipp-3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Make sure your notebook environment is try_mlserver\n",
    "%pip install -r assignment1/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's also install the mlserver packages. mlserver-mlflow is the out-of-box server implementation for MLflow models and boto3 is required by MLServer to load the model from the MLflow service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlserver==1.3.5\n",
      "  Downloading mlserver-1.3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting mlserver-mlflow==1.3.5\n",
      "  Downloading mlserver_mlflow-1.3.5-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting boto3~=1.28.80\n",
      "  Using cached boto3-1.28.85-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: click in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlserver==1.3.5) (8.1.7)\n",
      "Collecting fastapi!=0.89.0,<=0.89.1,>=0.88.0 (from mlserver==1.3.5)\n",
      "  Downloading fastapi-0.89.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv (from mlserver==1.3.5)\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting grpcio (from mlserver==1.3.5)\n",
      "  Downloading grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting importlib-resources (from mlserver==1.3.5)\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: numpy in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlserver==1.3.5) (1.26.0)\n",
      "Requirement already satisfied: pandas in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlserver==1.3.5) (1.5.3)\n",
      "Requirement already satisfied: protobuf in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlserver==1.3.5) (4.25.1)\n",
      "Collecting uvicorn (from mlserver==1.3.5)\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting starlette-exporter (from mlserver==1.3.5)\n",
      "  Downloading starlette_exporter-0.17.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting py-grpc-prometheus (from mlserver==1.3.5)\n",
      "  Downloading py_grpc_prometheus-0.7.0-py3-none-any.whl (12 kB)\n",
      "Collecting aiokafka (from mlserver==1.3.5)\n",
      "  Downloading aiokafka-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting tritonclient>=2.24 (from tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading tritonclient-2.39.0-py3-none-manylinux1_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting aiofiles (from mlserver==1.3.5)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting orjson (from mlserver==1.3.5)\n",
      "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvloop (from mlserver==1.3.5)\n",
      "  Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: mlflow in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlserver-mlflow==1.3.5) (2.3.0)\n",
      "Collecting botocore<1.32.0,>=1.31.85 (from boto3~=1.28.80)\n",
      "  Using cached botocore-1.31.85-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3~=1.28.80)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3~=1.28.80)\n",
      "  Using cached s3transfer-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.85->boto3~=1.28.80) (2.8.2)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore<1.32.0,>=1.31.85->boto3~=1.28.80)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 (from fastapi!=0.89.0,<=0.89.1,>=0.88.0->mlserver==1.3.5)\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette==0.22.0 (from fastapi!=0.89.0,<=0.89.1,>=0.88.0->mlserver==1.3.5)\n",
      "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5,>=3.4.0 (from starlette==0.22.0->fastapi!=0.89.0,<=0.89.1,>=0.88.0->mlserver==1.3.5)\n",
      "  Downloading anyio-4.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting cuda-python (from tritonclient>=2.24->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading cuda_python-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting python-rapidjson>=0.9.1 (from tritonclient>=2.24->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading python_rapidjson-1.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.1 (from tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting geventhttpclient<=2.0.2,>=1.4.4 (from tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading geventhttpclient-2.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout (from aiokafka->mlserver==1.3.5)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting kafka-python>=2.0.2 (from aiokafka->mlserver==1.3.5)\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from aiokafka->mlserver==1.3.5) (23.0)\n",
      "Requirement already satisfied: cloudpickle<3 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (2.2.1)\n",
      "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (0.18.0)\n",
      "Requirement already satisfied: entrypoints<1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (3.1.40)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /home/user/.local/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (6.0)\n",
      "Requirement already satisfied: pytz<2024 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (2.31.0)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (6.8.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (0.4.4)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (1.12.1)\n",
      "Requirement already satisfied: docker<7,>=4.0.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (6.1.3)\n",
      "Requirement already satisfied: Flask<3 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (2.3.3)\n",
      "Requirement already satisfied: scipy<2 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (1.10.1)\n",
      "Requirement already satisfied: querystring-parser<2 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (1.2.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (2.0.23)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (1.3.2)\n",
      "Requirement already satisfied: pyarrow<12,>=4.0.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (11.0.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (3.5.1)\n",
      "Requirement already satisfied: matplotlib<4 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (3.8.0)\n",
      "Requirement already satisfied: gunicorn<21 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (20.1.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/user/.local/lib/python3.10/site-packages (from mlflow->mlserver-mlflow==1.3.5) (3.1.2)\n",
      "Requirement already satisfied: setuptools>=39.0.1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from py-grpc-prometheus->mlserver==1.3.5) (68.0.0)\n",
      "Collecting prometheus-client>=0.3.0 (from py-grpc-prometheus->mlserver==1.3.5)\n",
      "  Downloading prometheus_client-0.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->mlserver==1.3.5)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from uvicorn->mlserver==1.3.5) (4.8.0)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading yarl-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: Mako in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow->mlserver-mlflow==1.3.5) (1.3.0)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow->mlserver-mlflow==1.3.5) (2.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow->mlserver-mlflow==1.3.5) (3.2.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow->mlserver-mlflow==1.3.5) (0.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow->mlserver-mlflow==1.3.5) (1.16.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from docker<7,>=4.0.0->mlflow->mlserver-mlflow==1.3.5) (1.6.4)\n",
      "Requirement already satisfied: Werkzeug>=2.3.7 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from Flask<3->mlflow->mlserver-mlflow==1.3.5) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from Flask<3->mlflow->mlserver-mlflow==1.3.5) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from Flask<3->mlflow->mlserver-mlflow==1.3.5) (1.7.0)\n",
      "Collecting gevent>=0.13 (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading gevent-23.9.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: certifi in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]>=2.24->mlserver==1.3.5) (2023.11.17)\n",
      "Collecting brotli (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow->mlserver-mlflow==1.3.5) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow->mlserver-mlflow==1.3.5) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/.local/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow->mlserver-mlflow==1.3.5) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from matplotlib<4->mlflow->mlserver-mlflow==1.3.5) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from matplotlib<4->mlflow->mlserver-mlflow==1.3.5) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from matplotlib<4->mlflow->mlserver-mlflow==1.3.5) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from matplotlib<4->mlflow->mlserver-mlflow==1.3.5) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from matplotlib<4->mlflow->mlserver-mlflow==1.3.5) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from matplotlib<4->mlflow->mlserver-mlflow==1.3.5) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow->mlserver-mlflow==1.3.5) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow->mlserver-mlflow==1.3.5) (3.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from scikit-learn<2->mlflow->mlserver-mlflow==1.3.5) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from scikit-learn<2->mlflow->mlserver-mlflow==1.3.5) (3.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow->mlserver-mlflow==1.3.5) (3.0.1)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi!=0.89.0,<=0.89.1,>=0.88.0->mlserver==1.3.5)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi!=0.89.0,<=0.89.1,>=0.88.0->mlserver==1.3.5) (1.0.4)\n",
      "Collecting zope.event (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting zope.interface (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]>=2.24->mlserver==1.3.5)\n",
      "  Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /home/user/anaconda3/envs/try_mlserver/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow->mlserver-mlflow==1.3.5) (5.0.1)\n",
      "Downloading mlserver-1.3.5-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlserver_mlflow-1.3.5-py3-none-any.whl (10 kB)\n",
      "Using cached boto3-1.28.85-py3-none-any.whl (135 kB)\n",
      "Using cached botocore-1.31.85-py3-none-any.whl (11.3 MB)\n",
      "Using cached s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "Downloading tritonclient-2.39.0-py3-none-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiokafka-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette_exporter-0.17.1-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading prometheus_client-0.19.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_rapidjson-1.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Downloading cuda_python-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.1.0-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gevent-23.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kafka-python, cuda-python, brotli, zope.interface, zope.event, uvloop, urllib3, sniffio, python-rapidjson, python-dotenv, pydantic, prometheus-client, orjson, multidict, jmespath, importlib-resources, h11, grpcio, frozenlist, attrs, async-timeout, aiofiles, yarl, uvicorn, tritonclient, py-grpc-prometheus, gevent, botocore, anyio, aiosignal, aiokafka, starlette, s3transfer, geventhttpclient, aiohttp, starlette-exporter, fastapi, boto3, mlserver, mlserver-mlflow\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "Successfully installed aiofiles-23.2.1 aiohttp-3.9.1 aiokafka-0.8.1 aiosignal-1.3.1 anyio-4.1.0 async-timeout-4.0.3 attrs-23.1.0 boto3-1.28.85 botocore-1.31.85 brotli-1.1.0 cuda-python-12.3.0 fastapi-0.89.1 frozenlist-1.4.0 gevent-23.9.1 geventhttpclient-2.0.2 grpcio-1.59.3 h11-0.14.0 importlib-resources-6.1.1 jmespath-1.0.1 kafka-python-2.0.2 mlserver-1.3.5 mlserver-mlflow-1.3.5 multidict-6.0.4 orjson-3.9.10 prometheus-client-0.19.0 py-grpc-prometheus-0.7.0 pydantic-1.10.13 python-dotenv-1.0.0 python-rapidjson-1.13 s3transfer-0.7.0 sniffio-1.3.0 starlette-0.22.0 starlette-exporter-0.17.1 tritonclient-2.39.0 urllib3-2.0.7 uvicorn-0.24.0.post1 uvloop-0.19.0 yarl-1.9.3 zope.event-5.0 zope.interface-6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mlserver==1.3.5 mlserver-mlflow==1.3.5 boto3~=1.28.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1 instructions\n",
    "Now you need to \n",
    "1. Add configurations to the empty [assignment1/model-settings.json](./assignment1/model-settings.json) to use the MLServer's MLflow runtime to serve your LightGBM model. The inference service name should be ***bike-demand-predictor***. The configuration can be adapted from the [one provided by this MLServer doc.](https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html#serving)\n",
    "1. In a separate terminal (where the \"try_mlserver\" conda environment is activated), start an MLServer inference service to serve the LightGBM model.\n",
    "1. Keep the inference service running and use the following code cell to check whether or not your configuration works. **Please keep the output of the following code cells.** The reviewer will use the output to check your MLServer configuration. \n",
    "\n",
    "Notes:\n",
    "- When starting an MLServer inference service in a terminal, you need to change the conda environment of that terminal session to \"try_mlserver\" by running `conda activate try_mlserver` so that the mlserver command can be found. \n",
    "\n",
    "- MLServer will load the model from your MinIO storage service so you need to specify the following environment variables to allow MLServer to use the correct credentials to load the model from the correct MinIO service endpoint:\n",
    "```bash\n",
    "# Run the following command in a terminal\n",
    "export AWS_ACCESS_KEY_ID=minioadmin\n",
    "export AWS_SECRET_ACCESS_KEY=minioadmin\n",
    "export MLFLOW_S3_ENDPOINT_URL=http://mlflow-minio.local\n",
    "```\n",
    "These environment variables are only available in the terminal session where you defined them, so you need to start your MLServer inference service in the same terminal session where you defined the above environment variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from mlserver.codecs import PandasCodec\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# We need to use some functions in ../train_helper.py so we just append it to the Python path at runtime. \n",
    "# The Python path is a list of directory locations where Python looks for modules and packages when you try to import them in your code.\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from train_helper import pull_data, preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parameters': {'content_type': 'pd'}, 'inputs': [{'name': 'season', 'shape': [5, 1], 'datatype': 'INT64', 'data': [4, 4, 4, 4, 4]}, {'name': 'holiday', 'shape': [5, 1], 'datatype': 'INT64', 'data': [0, 0, 0, 0, 0]}, {'name': 'workingday', 'shape': [5, 1], 'datatype': 'INT64', 'data': [1, 1, 1, 1, 1]}, {'name': 'weather', 'shape': [5, 1], 'datatype': 'INT64', 'data': [2, 2, 2, 2, 2]}, {'name': 'temp', 'shape': [5, 1], 'datatype': 'FP64', 'data': [11.48, 11.48, 10.66, 10.66, 10.66]}, {'name': 'atemp', 'shape': [5, 1], 'datatype': 'FP64', 'data': [13.635, 12.88, 12.12, 12.12, 12.88]}, {'name': 'humidity', 'shape': [5, 1], 'datatype': 'INT64', 'data': [52, 52, 56, 56, 56]}, {'name': 'windspeed', 'shape': [5, 1], 'datatype': 'FP64', 'data': [15.0013, 19.0012, 16.9979, 19.0012, 12.998]}, {'name': 'hour', 'shape': [5, 1], 'datatype': 'INT64', 'data': [0, 1, 2, 3, 4]}, {'name': 'day', 'shape': [5, 1], 'datatype': 'INT64', 'data': [13, 13, 13, 13, 13]}, {'name': 'month', 'shape': [5, 1], 'datatype': 'INT64', 'data': [12, 12, 12, 12, 12]}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_url = \"https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv\"\n",
    "\n",
    "# Prepare some data to send in requests\n",
    "df = pull_data(dataset_url)\n",
    "_, test = preprocess(df)\n",
    "test_x = test.drop([\"count\"], axis=1)\n",
    "\n",
    "request_data = test_x.head()\n",
    "\n",
    "# Encode the request data following the V2 inference protocol\n",
    "encoded_request_data = PandasCodec.encode_request(request_data).dict()\n",
    "print(encoded_request_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'bike-demand-predictor',\n",
       " 'id': '42914e0e-81df-4cff-96d2-1c323af90e8b',\n",
       " 'parameters': {'content_type': 'np'},\n",
       " 'outputs': [{'name': 'output-1',\n",
       "   'shape': [5, 1],\n",
       "   'datatype': 'FP64',\n",
       "   'parameters': {'content_type': 'np'},\n",
       "   'data': [37.289116222680455,\n",
       "    19.406971833185164,\n",
       "    10.248384070712056,\n",
       "    9.602077884278172,\n",
       "    9.602077884278172]}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send a request\n",
    "response = requests.post(\"http://localhost:8080/v2/models/bike-demand-predictor/infer\", json=encoded_request_data)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{'model_name': 'bike-demand-predictor',\n",
    " 'id': 'f021577e-16fb-4686-8f1e-70f3ae2a7b76',\n",
    " 'parameters': {'content_type': 'np'},\n",
    " 'outputs': [{'name': 'output-1',\n",
    "   'shape': [5, 1],\n",
    "   'datatype': 'FP64',\n",
    "   'parameters': {'content_type': 'np'},\n",
    "   'data': [42.09539399666931,\n",
    "    23.974666238188583,\n",
    "    13.463013296846174,\n",
    "    8.769204532023744,\n",
    "    8.769204532023744]}]}\n",
    "```\n",
    "The id may vary. The output data ([42.09..., 23.97..., ...]) may also vary depending on how you trained the model in the first week. The key point is that the response should follow the same format as the expected output. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, you need to **switch the notebook's environment back to \"mlops_eng\"** for the rest of the assignments. \n",
    "\n",
    "### Guidelines for doing Assignments 2-5\n",
    "- In 2a), you'll need to write some Python code, so please put your code between the `### START CODE HERE` and `### END CODE HERE` comments. \n",
    "- In other assignments, you'll need to complete some configurations in given YAML files. Please write your configurations between the `### START CONF HERE` and `### END CONF HERE` comments in each YAML file.\n",
    "- You will use a command `kubectl -n kserve-inference get isvc <name-of-inference-service>` (or `kubectl -n kserve-inference get ig <name-of-inference-graph>`) a few times when running this notebook. This command checks whether your inference service (or inference graph) deployed to KServe is ready. It takes some time (up to a few minutes) for a inference service/graph to become ready, so you may need to run the same command a few times to follow the readiness of your inference service/graph. You can also use the \"-w\" option to continuously watch the status of the inference service/graph (`kubectl get isvc <name-of-inference-service> -n kserve-inference -w`) and then terminate the code cell when the inference service/graph is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Deploy a model to KServe (3 points)\n",
    "In this assignment, you need to deploy your LightGBM model for predicting bike sharing demand as an inference service to KServe. You can use the model trained in the first week. \n",
    "\n",
    "Similar to the tutorial, the deployed inference service should run in the \"kserve-inference\" namespace and the service account name containing the credentials for accessing the MinIO storage service is also \"kserve-sa\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1ModelSpec\n",
    "from kserve import V1beta1ModelFormat\n",
    "import logging\n",
    "\n",
    "from send_request import send_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Use Python SDK to deploy your LightGBM model\n",
    "Complete the `deploy_model` function that uses the KServe SDK to deploy your LightGBM model. \n",
    "\n",
    "**Hint**: Using the LightGBM server provided by KServe doesn't work because the model saved by MLflow is in the pickled format, which is different from the format supported by KServe's LightGBM server. You can check [here](https://github.com/kserve/kserve/issues/2483) on how to use KServe SDK to deploy a model uploaded to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(model_name: str, model_uri: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name: the name of the deployed inference service\n",
    "        model_uri: the S3 URI of the model saved in MLflow\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    namespace = \"kserve-inference\"\n",
    "    service_account_name=\"kserve-sa\"\n",
    "    kserve_version=\"v1beta1\"\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + kserve_version\n",
    "    \n",
    "    logger.info(f\"MODEL URI: {model_uri}\")\n",
    "    \n",
    "    modelspec = V1beta1ModelSpec(\n",
    "        storage_uri=model_uri,\n",
    "        model_format=V1beta1ModelFormat(name=\"mlflow\"),\n",
    "        protocol_version=\"v2\"\n",
    "    )\n",
    "    \n",
    "    isvc = V1beta1InferenceService(\n",
    "        ### START CODE HERE\n",
    "        # define api_version, kind, and metadata\n",
    "\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=model_name,\n",
    "            namespace=namespace,\n",
    "        ),\n",
    "        ### END CODE HERE\n",
    "\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                ### START CODE HERE\n",
    "                service_account_name = service_account_name,\n",
    "                model=modelspec\n",
    "                ### END CODE HERE\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    kserve = KServeClient()\n",
    "\n",
    "    ### START CODE HERE\n",
    "    # Create or update an inference service\n",
    "    try:\n",
    "        kserve.create(inferenceservice=isvc)\n",
    "    except RuntimeError:\n",
    "        # If the inference service with the same name exists\n",
    "        kserve.patch(name=model_name, inferenceservice=isvc, namespace=namespace)\n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 21:34:17.680 28238 __main__ INFO [deploy_model():16] MODEL URI: s3://mlflow/5/c946adf98fd348a1bf993121f8133893/artifacts/model\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bike-lgbm-2a\"\n",
    "\n",
    "# Replace the storage_uri to your own one\n",
    "model_uri = \"s3://mlflow/5/c946adf98fd348a1bf993121f8133893/artifacts/model\"\n",
    "\n",
    "# Test the deploy_model function\n",
    "deploy_model(model_name, model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                    AGE\n",
      "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-default-00001   2m36s\n"
     ]
    }
   ],
   "source": [
    "# Check if the \"bike-lgbm-2a\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                    AGE\n",
    "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-default-00001   72s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-2a-predictor-default-00001-deployment-749d894d8977fjj   2/2     Running   0          2m43s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there is one pod running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-2a-predictor-default-00001-deployment-6499598b7-wc28j   2/2     Running   0          65s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bike-lgbm-2a', 'model_version': None, 'id': 'c7f8435f-96b1-4378-8b4b-8d9b4deb74af', 'parameters': None, 'outputs': [{'name': 'predict', 'shape': [2], 'datatype': 'FP64', 'parameters': None, 'data': [51.00457318737209, 35.13687405851507]}]}\n"
     ]
    }
   ],
   "source": [
    "# Send a request to the inference service\n",
    "send_request(model_name=\"bike-lgbm-2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{'model_name': 'bike-lgbm-2a', \n",
    "'model_version': None, \n",
    "'id': '4a91cc4c-3a04-4aa1-95ee-6bbbf04207b7', \n",
    "'parameters': None, \n",
    "'outputs': [{'name': 'predict', 'shape': [2], 'datatype': 'FP64', 'parameters': None, 'data': [51.00457318737209, 35.13687405851507]}]\n",
    "}\n",
    "```\n",
    "**Note**: The id varies. The output data ([51.0..., 35.1...]) may also vary depending on how your model was trained. The important point is that the response has the correct fields as shown in the above expected output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S.* KServe also uses MLServer to serve the models uploaded to the MLflow service, which means your inference service uses V2 inference protocol. If you take a look at the `send_request` function in [send_request.py](./send_request.py), you'll observe that the input data formats and the URL format align with what was used in Assignment 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm-2a\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up by removing the \"bike-lgbm-2a\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-2a\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Use a YAML file to deploy the model\n",
    "Instead of using the KServe SDK, now you need to use a YAML file to deploy your LightGBM model again. Please complete the configuration in [manifests/bike-lgbm-basic.yaml](./manifests/bike-lgbm-basic.yaml).\n",
    "\n",
    "**Hint**: You can check from [this KServe doc](https://kserve.github.io/website/0.10/modelserving/v1beta1/mlflow/v2/#deploy-with-inferenceservice) on how to use a YAML manifest to deploy a model stored in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the LightGBM model for bike demand prediction as an inference service named \"bike-lgbm\"\n",
    "!kubectl apply -f manifests/bike-lgbm-basic.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
      "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   26s\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the \"bike-lgbm\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   2m24s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                            READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-predictor-default-00001-deployment-6db5d588c6-45787   2/2     Running   0          43s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there is one pod running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-predictor-default-00001-deployment-9d7b87595-k9kpk   2/2     Running   0          70s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bike-lgbm', 'model_version': None, 'id': 'ed780fa1-8f47-4596-a0bb-ceac8be944db', 'parameters': None, 'outputs': [{'name': 'predict', 'shape': [2], 'datatype': 'FP64', 'parameters': None, 'data': [51.00457318737209, 35.13687405851507]}]}\n"
     ]
    }
   ],
   "source": [
    "# Send some requests to the \"bike-lgbm\" inference service\n",
    "send_request(model_name=\"bike-lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{'model_name': 'bike-lgbm', \n",
    "'model_version': None, \n",
    "'id': '6783fc56-a759-41b0-9d01-94be32238b01', \n",
    "'parameters': None, \n",
    "'outputs': [{'name': 'predict', 'shape': [2], 'datatype': 'FP64', 'parameters': None, 'data': [51.00457318737209, 35.13687405851507]}]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please don't delete the \"bike-lgbm\" inference service, you will need it in Assignment3 later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Canary deployment in KServe (2 points)\n",
    "You'll train a new LightGBM model for predicting the bike sharing demand. In this assignment, your task is to deploy the new model to KServe using the canary deployment strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to make sure there's already a \"bike-lgbm\" inference service running in KServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   47m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a new LightGBM model and upload it to MLflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new LightGBM model for predicting bike sharing demand\n",
    "# This is the same as the assignment in Week1, we just change the hyperparameters of the model and skip the part that uses Deepchecks to perform offline model evaluation\n",
    "# The model's S3 URI will be printed at the end\n",
    "from train_helper import train\n",
    "\n",
    "params = {\n",
    "    \"num_leaves\": 127, # Before it was 63\n",
    "    \"learning_rate\": 0.1, # Before it was 0.05\n",
    "    \"random_state\": 42 \n",
    "}\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to complete the configuration in [manifests/bike-lgbm-canary.yaml](./manifests/bike-lgbm-canary.yaml) to deploy the new LightGBM model using canary deployment. Your new model should receive **30%** of the user traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the \"bike-lgbm\" inference service to use the new model\n",
    "!kubectl apply -f manifests/bike-lgbm-canary.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm configured\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the traffic is splitted between the old and the new version\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION               LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True    70     30       bike-lgbm-predictor-default-00001   bike-lgbm-predictor-default-00002   61m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are two pods (one old and one new one) running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-predictor-default-00001-deployment-cc96598f-rr2xz    2/2     Running   0          63m\n",
    "bike-lgbm-predictor-default-00002-deployment-6d9f5bbff-8mhn8   2/2     Running   0          3m36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by removing the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Horizontal autoscaling (2 points)\n",
    "\n",
    "In this assignment, your task is to complete the configuration in [manifests/bike-lgbm-scale.yaml](./manifests/bike-lgbm-scale.yaml) to deploy your LightGBM model to KServe and configure the horizontal autoscaling feature for the deployed inference service. Specifically, the horizontal autoscaling of the inference service should satisfy the following requirements:\n",
    "1. The inference service should have ae least **2** pods running;\n",
    "2. The inference service can have at most **8** pods running when it's being scaled up;\n",
    "3. The inference service should be scaled up when each pod is receiving no less than 5 requests per second.\n",
    "\n",
    "You can use whichever LightGBM model you trained for predicting bike sharing demand in this assignment. \n",
    "\n",
    "**Hint**: \"rps\" should be used as the scaling metric. \n",
    "\n",
    "*rps (requests per second) VS concurrency: These two metrics may look similar at the first glance. Both of them are metrics used to measure service performance. rps quantifies the number of requests a service can process within a specific time frame, often a second, whereas concurrency focuses on how many tasks a service can handle simultaneously.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy an inference service named \"bike-lgbm-scale\"\n",
    "!kubectl apply -f manifests/bike-lgbm-scale.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-scale created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the \"bike-lgbm-scale\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME              URL                                                   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                       AGE\n",
    "bike-lgbm-scale   http://bike-lgbm-scale.kserve-inference.example.com   True           100                              bike-lgbm-scale-predictor-default-00001   90s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are two pods (replicas) running for the \"bike-lgbm-scale\" inference service\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-scale-predictor-default-00001-deployment-66df7bcd67mr   2/2     Running   0          7m26s\n",
    "bike-lgbm-scale-predictor-default-00001-deployment-66df7bcjb6qx   2/2     Running   0          7m27s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now please use the command below to print the configuration of your \"bike-lgbm-scale\" inference service. The output will be used to check if your configuration is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl -n kserve-inference get isvc bike-lgbm-scale -o json|jq .spec.predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by removing the \"bike-lgbm-scale\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-scale\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Inference graph in KServe (3 points)\n",
    "\n",
    "## 5a) Inference graph for ensemble\n",
    "So far you already have two LightGBM models for predicting bike sharing demand. One was trained using the hyperparameters of {learning_rate=0.05, num_leaves=63} (denoted by Model A) and another {learning_rate=0.1, num_leaves=127} (denoted by Model B). \n",
    "\n",
    "You need to first complete the configuration in [manifests/bike-lgbm-graph1.yaml](./manifests/bike-lgbm-graph1.yaml) to deploy two inference services named \"bike-lgbm-1\" and \"bike-lgbm-2\". The \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services should serve Model A and B, respectively.\n",
    "\n",
    "Next, you need to complete [manifests/inference-graph1.yaml](./manifests/inference-graph1.yaml) to deploy an inference graph that includes one ensemble routing node. With this inference graph, a user will receive two predictions (one from each inference service) when they send a request.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services\n",
    "!kubectl apply -f manifests/bike-lgbm-graph1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-1 created\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the two inference services are ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-1 bike-lgbm-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-default-00001   105m\n",
    "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-default-00001   105m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are two pods running for the two inference services, respectively\n",
    "!kubectl -n kserve-inference get pods -l \"serving.kserve.io/inferenceservice in (bike-lgbm-1,bike-lgbm-2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-1-predictor-default-00001-deployment-794547df56-48dhk   2/2     Running   0          109m\n",
    "bike-lgbm-2-predictor-default-00001-deployment-cf7b449b5-rjc2q    2/2     Running   0          109m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the inference graph named \"my-graph1\"\n",
    "!kubectl apply -f manifests/inference-graph1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/my-graph1 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the \"my-graph1\" inference graph is ready\n",
    "!kubectl -n kserve-inference get ig my-graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   AGE\n",
    "my-graph1   http://my-graph1.kserve-inference.example.com   True    102s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also make sure there is one pod running for the \"ensemble\" inference graph\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferencegraph=my-graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                          READY   STATUS    RESTARTS   AGE\n",
    "my-graph1-00001-deployment-7c4d7cfbf9-tr5tz   2/2     Running   0          2m7s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's send a request to the \"my-graph1\" inference graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request\n",
    "from send_request import send_request\n",
    "\n",
    "send_request(to_ig=True, ig_name=\"my-graph1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output (i.e., the response) is expected to contain a prediction from the \"bike-lgbm-1\" inference service and another prediction from the \"bike-lgbm-2\" inference service. \n",
    "\n",
    "Example output:\n",
    "```text\n",
    "{'bike-lgbm-v1': {'id': 'ec476473-c736-478d-9830-e2b6f53548db', 'model_name': 'bike-lgbm-1', 'model_version': None, 'outputs': [{'data': [51.00457318737209, 35.13687405851507], 'datatype': 'FP64', 'name': 'predict', 'parameters': None, 'shape': [2]}], 'parameters': None}, 'bike-lgbm-v2': {'id': 'fd17e0c6-d17a-42e0-8cbe-d452e0107d34', 'model_name': 'bike-lgbm-2', 'model_version': None, 'outputs': [{'data': [34.87125805456099, 32.68341881111533], 'datatype': 'FP64', 'name': 'predict', 'parameters': None, 'shape': [2]}], 'parameters': None}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Do not delete the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services. They're still needed in the next assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b) More complicated inference graph\n",
    "In this assignment, you need to deploy a more complex inference graph containing more than one routing node. \n",
    "\n",
    "First let's train the third model, denoted by Model C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_helper import train\n",
    "\n",
    "params = {\n",
    "    \"num_leaves\": 127, \n",
    "    \"learning_rate\": 0.17,\n",
    "    \"min_child_samples\": 50,\n",
    "    \"random_state\": 42 \n",
    "}\n",
    "\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Assignment 5a, your tasks are to\n",
    "1. Complete [manifests/bike-lgbm-graph2.yaml](./manifests/bike-lgbm-graph2.yaml) to deploy third inference service named \"bike-lgbm-3\" that serves Model C you just trained. \n",
    "2. Complete [manifests/inference-graph2.yaml](./manifests/inference-graph2.yaml) to deploy an inference graph containing a Switch and a Ensemble routing node. \n",
    "The requests that will be sent to the inference graph look like:\n",
    "    ```python\n",
    "    {\n",
    "      'inputs': ...,\n",
    "      'userType': 'basic'\n",
    "    }\n",
    "    ```\n",
    "    The inference graph should satisfy the following requirements:\n",
    "    - If there is a field named \"userType\" in the request and its value is \"basic\", the request should be forwarded to an ensemble consisting of the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services. At this time, the \"basic\" user should receive one prediction from the \"bike-lgbm-1\" inference service and another prediction from the \"bike-lgbm-2\" inference service, just like in the previous assignment. \n",
    "    - If the value of \"userType is \"advanced\", the request should be forwarded to the \"bike-lgbm-3\" inference service. At this time, the \"advanced\" user should receive one prediction from the \"bike-lgbm-3\" inference service.\n",
    "    - Otherwise the request should be directly returned. \n",
    "    \n",
    "    The behavior of the inference graph is illustrated in the figure below:\n",
    "\n",
    "    <img src=\"./images/complex-inference-graph.jpg\" width=600/>\n",
    "\n",
    "**Hints**:\n",
    "You may notice that you need to route requests from one routing node to another (instead of from a routing node to an inference service). Below is an example of configuring a routing node to forward requests to another routing node:\n",
    "```yaml\n",
    "...\n",
    "spec: \n",
    "  nodes: \n",
    "    # The first routing node\n",
    "    root: \n",
    "      routerType: ...\n",
    "      steps: \n",
    "      # This routing node forwards requests to the second routing node named \"ensembleNode\"\n",
    "      - nodeName: ensembleNode\n",
    "\n",
    "    # The second routing node\n",
    "    ensembleNode:\n",
    "      routerType: ...\n",
    "      steps:\n",
    "      ...\n",
    "```\n",
    "You can use `\"[@this].#(userType==\\\"...\\\")\"` as the condition that determines whether a request should be routed to an ensemble of model A and model B, or to to the standalone model C.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the third inference service named \"bike-lgbm-3\"\n",
    "!kubectl apply -f manifests/bike-lgbm-graph2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-3 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the all of the three inference services are ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-1 bike-lgbm-2 bike-lgbm-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-default-00001   132m\n",
    "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-default-00001   132m\n",
    "bike-lgbm-3   http://bike-lgbm-3.kserve-inference.example.com   True           100                              bike-lgbm-3-predictor-default-00001   22s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are three pods running for the three inference services, respectively\n",
    "!kubectl -n kserve-inference get pods -l \"serving.kserve.io/inferenceservice in (bike-lgbm-1,bike-lgbm-2,bike-lgbm-3)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-1-predictor-default-00001-deployment-794547df56-48dhk   2/2     Running   0          132m\n",
    "bike-lgbm-2-predictor-default-00001-deployment-cf7b449b5-rjc2q    2/2     Running   0          132m\n",
    "bike-lgbm-3-predictor-default-00001-deployment-7684784979-zmmgw   2/2     Running   0          42s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the second inference graph (\"my-graph2\")\n",
    "!kubectl apply -f manifests/inference-graph2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/my-graph2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the inference graph named \"my-graph2\" is ready\n",
    "!kubectl -n kserve-inference get ig my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   AGE\n",
    "my-graph2   http://my-graph2.kserve-inference.example.com   True    35s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also make sure there is one pod running for the \"ensemble\" inference graph\n",
    "!kubectl -n kserve-inference get pods -l  serving.kserve.io/inferencegraph=my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME                                          READY   STATUS    RESTARTS   AGE\n",
    "my-graph2-00001-deployment-557678dfbd-zglcg   2/2     Running   0          49s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send some requests\n",
    "from send_request import send_request\n",
    "send_request(to_ig=True, ig_name=\"my-graph2\", user_type=\"basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is expected to contain predictions from both \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```text\n",
    "{'bike-lgbm-v1': {'id': '03f3bc30-a628-418f-9539-2372f66d353f', 'model_name': 'bike-lgbm-1', 'model_version': None, 'outputs': [{'data': [51.00457318737209, 35.13687405851507], 'datatype': 'FP64', 'name': 'predict', 'parameters': None, 'shape': [2]}], 'parameters': None}, 'bike-lgbm-v2': {'id': 'a8a68d72-03b4-4b2b-91a9-3e5a08cbefbd', 'model_name': 'bike-lgbm-2', 'model_version': None, 'outputs': [{'data': [34.87125805456099, 32.68341881111533], 'datatype': 'FP64', 'name': 'predict', 'parameters': None, 'shape': [2]}], 'parameters': None}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_request(to_ig=True, ig_name=\"my-graph2\", user_type=\"advanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response should only have predictions from the \"bike-lgbm-3\" inference service.\n",
    "\n",
    "Example output:\n",
    "```text\n",
    "{'model_name': 'bike-lgbm-3', 'model_version': None, 'id': '6bfde99b-e23a-47cd-8192-a1cdc0773c4a', 'parameters': None, 'outputs': [{'name': 'predict', 'shape': [2], 'datatype': 'FP64', 'parameters': None, 'data': [43.59313309843417, 32.17377957904267]}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_request(to_ig=True, ig_name=\"my-graph2\", user_type=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request should be directly returned.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```text\n",
    "{'parameters': {'content_type': 'pd'}, 'inputs': [{'name': 'season', 'shape': [2], 'datatype': 'UINT64', 'data': [1, 1]}, {'name': 'holiday', 'shape': [2], 'datatype': 'UINT64', 'data': [0, 0]}, {'name': 'workingday', 'shape': [2], 'datatype': 'UINT64', 'data': [0, 0]}, {'name': 'weather', 'shape': [2], 'datatype': 'UINT64', 'data': [1, 1]}, {'name': 'temp', 'shape': [2], 'datatype': 'FP64', 'data': [9.84, 9.02]}, {'name': 'atemp', 'shape': [2], 'datatype': 'FP64', 'data': [14.395, 13.635]}, {'name': 'humidity', 'shape': [2], 'datatype': 'UINT64', 'data': [81, 80]}, {'name': 'windspeed', 'shape': [2], 'datatype': 'FP64', 'data': [0.0, 0.0]}, {'name': 'hour', 'shape': [2], 'datatype': 'UINT64', 'data': [0, 1]}, {'name': 'day', 'shape': [2], 'datatype': 'UINT64', 'data': [1, 1]}, {'name': 'month', 'shape': [2], 'datatype': 'UINT64', 'data': [1, 1]}], 'userType': 'random'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all of the three inference services\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-1 bike-lgbm-2 bike-lgbm-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-1\" deleted\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-2\" deleted\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-3\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all inference graphs\n",
    "!kubectl -n kserve-inference delete ig my-graph1 my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferencegraph.serving.kserve.io \"my-graph1\" deleted\n",
    "inferencegraph.serving.kserve.io \"my-graph2\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Please make sure you have the following files in your submission:\n",
    "- This notebook (with up-to-date outputs of the code cells)\n",
    "- model-settings.json \n",
    "- all YAML files listed in the \"manifests\" directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
